{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Prediction of Sharpe ratio for blends of quantitative strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "hidden": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/Training_Input_2dx8C9Q.csv\")\n",
    "target = pd.read_csv(\"Data/Training_Output_IJhBXtA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"Data/Testing_Input_dPKY3Rf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-12.007941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.294867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.652308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.412364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.517471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Target\n",
       "ID           \n",
       "0  -12.007941\n",
       "1    2.294867\n",
       "2    0.652308\n",
       "3    2.412364\n",
       "4    8.517471"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data = data.set_index(\"ID\")\n",
    "target = target.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "test = test.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Missing values ?\n",
    "print(True in data.isna(), True in target.isna(), True in test.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's see the columns to understand the construction of the dataset. \n",
    "# It begins with lag_20 and finishes with lag_0 for the 10 time series. \n",
    "# So lag_20 represent the fist day of a month set to a value of \"100\" of investment.\n",
    "for i in data.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# There is up to 50 different samples (different weights) for the same time serie of 26 days \n",
    "# (21 days of training and 5 days of prediction)\n",
    "# given a train dataset of 10 000 samples, if there are exactly 50 differents, we should obtain :\n",
    "# 10 000/50 = 200 unique (values) time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 200\n",
      "1 200\n",
      "2 200\n",
      "3 200\n",
      "4 200\n",
      "5 200\n",
      "6 200\n",
      "7 200\n",
      "8 199\n",
      "9 199\n",
      "10 199\n",
      "11 198\n",
      "12 197\n",
      "13 197\n",
      "14 196\n",
      "15 194\n",
      "16 193\n",
      "17 193\n",
      "18 192\n",
      "19 182\n",
      "20 1\n"
     ]
    }
   ],
   "source": [
    "# I think any strategy I or macro feature X could allow to find the number of unique values,\n",
    "# because for 20 different variations over the 21 days, only the same variations can give a same final value\n",
    "# there is no place for hazard here. \n",
    "\n",
    "# For strategy I_1 (for example) we can find our same 200 time series starting at lag_7 (14th day of the month)\n",
    "for i in range(0,21):\n",
    "    print(i,data[\"I_1_lag_\"+str(i)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 89\n",
      "1 89\n",
      "2 89\n",
      "3 89\n",
      "4 89\n",
      "5 89\n",
      "6 89\n",
      "7 88\n",
      "8 87\n",
      "9 87\n",
      "10 86\n",
      "11 86\n",
      "12 84\n",
      "13 84\n",
      "14 83\n",
      "15 82\n",
      "16 80\n",
      "17 80\n",
      "18 79\n",
      "19 77\n",
      "20 1\n"
     ]
    }
   ],
   "source": [
    "# Let's check for the test dataset with I_7 (for example) \n",
    "for i in range(0,21):\n",
    "    print(i,test[\"I_7_lag_\"+str(i)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Same, i find the 89 unique values expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating the customized cross_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# A difficulty here is that the different samples for a same time serie are shuffled into the dataset. \n",
    "# So I can not just randomly split the dataset into a train/val datasets. \n",
    "# I have to make sure to have different time series in train and validation.\n",
    "# So let's make our own cross_val. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 289\n",
      "1 289\n",
      "2 289\n",
      "3 289\n",
      "4 285\n",
      "5 282\n",
      "6 280\n",
      "7 278\n",
      "8 277\n",
      "9 270\n",
      "10 266\n",
      "11 266\n",
      "12 262\n",
      "13 257\n",
      "14 255\n",
      "15 253\n",
      "16 250\n",
      "17 248\n",
      "18 242\n",
      "19 233\n",
      "20 1\n"
     ]
    }
   ],
   "source": [
    "# First, let's check if there are same time series in train and test \n",
    "# We have 200 unique values for train, 89 for test, if indeed time series are different\n",
    "# we should obtain 289 unique values in the concatenation oh both. \n",
    "check = pd.concat([data, test])\n",
    "for i in range(0,21):\n",
    "    print(i,check[\"I_7_lag_\"+str(i)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we now know train and test are not shuffled and correspond to different time series.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Let's split train/val by values of I1_lag_0 (for example), as there are 200 unique values\n",
    "values = data.I_1_lag_0.unique()\n",
    "train, val = train_test_split(values, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "full_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# I add the target to split its values the same way as for features. \n",
    "full_data[\"target\"] = target.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# By filtering on train and val unique values, I can apply the split on the entire dataset\n",
    "df_train = full_data[data.I_1_lag_0.isin(train)]\n",
    "df_val = full_data[data.I_1_lag_0.isin(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7500 entries, 0 to 9999\n",
      "Columns: 218 entries, weight_I_1 to target\n",
      "dtypes: float64(218)\n",
      "memory usage: 12.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2500 entries, 4 to 9996\n",
      "Columns: 218 entries, weight_I_1 to target\n",
      "dtypes: float64(218)\n",
      "memory usage: 4.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop([\"target\"],axis=1)\n",
    "y_train = df_train.loc[:,[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_val = df_val.drop([\"target\"],axis=1)\n",
    "y_val = df_val.loc[:,[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "y_train[\"smoothed\"] = np.sign(y_train[\"target\"])*np.exp(-1/abs(y_train[\"target\"]))\n",
    "y_val[\"smoothed\"] = np.sign(y_val[\"target\"])*np.exp(-1/abs(y_val[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "target[\"smoothed\"] = np.sign(target[\"Target\"])*np.exp(-1/abs(target[\"Target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's recreate the benchmark score\n",
    "# The benchmark is the average Sharpe ratio of the training period\n",
    "# The metric is a L1 norm with values smoothed by a fonction : f(x) = sig(x)*exp(-1/abs(x))\n",
    "# The challenge doesn't specify if the train period taken is the entire train period or \n",
    "# the train period after cross_val split. \n",
    "# I choose to take the entire train period. However, this is not really important for the results. \n",
    "\n",
    "#avg = y_train.target.mean()\n",
    "avg = target.Target.mean()\n",
    "X_train_benchmark = X_train.copy()\n",
    "X_val_benchmark = X_val.copy()\n",
    "X_train_benchmark[\"pred_benchmark\"] = np.sign(avg)*np.exp(-1/abs(avg))\n",
    "X_val_benchmark[\"pred_benchmark\"] = np.sign(avg)*np.exp(-1/abs(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Sharpe ratio over the training set is : 1.2883218600109478\n",
      "Benchmark train score: 0.5953819474275232, Benchmark validation score: 0.5936898815943492\n"
     ]
    }
   ],
   "source": [
    "benchmark_train_score = mean_absolute_error(y_train[\"smoothed\"], X_train_benchmark[\"pred_benchmark\"])\n",
    "benchmark_val_score =  mean_absolute_error(y_val[\"smoothed\"], X_val_benchmark[\"pred_benchmark\"])\n",
    "print(\"The average Sharpe ratio over the training set is :\",avg)\n",
    "print(\"Benchmark train score: {}, Benchmark validation score: {}\".format(benchmark_train_score, benchmark_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(max_depth=2, criterion=\"mae\", random_state=2)\n",
    "rfr_2 = RandomForestRegressor(max_depth=2, criterion=\"mae\", random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 58s, sys: 1.91 s, total: 4min\n",
      "Wall time: 4min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mae',\n",
       "                      max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=2, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rfr.fit(X_train_4, y_train.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_pred_train = rfr.predict(X_train_4)\n",
    "rfr_pred_val = rfr.predict(X_val_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rfr_pred_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-9774dbd1ed1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rf_pred\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfr_pred_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_val_4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rf_pred\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfr_pred_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rfr_pred_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_4[\"pred\"] = rfr_pred_train\n",
    "X_val_4[\"pred\"] = rfr_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5319793305387601\n",
      "0.48690973394561204\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(X_train_4.pred, y_train.smoothed))\n",
    "print(mean_absolute_error(X_val_4.pred, y_val.smoothed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model on the entire train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mae',\n",
       "                      max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=2, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_2.fit(data_4, target.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_full_train = rfr_2.predict(data_4)\n",
    "rf_pred = rfr_2.predict(test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4[\"pred\"] = rf_pred_full_train\n",
    "test_4[\"pred\"] = rf_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4[\"fpred\"] = np.sign(data_4['pred']) * np.exp(-1/abs(data_4['pred']))\n",
    "test_4[\"fpred\"] = np.sign(test_4['pred']) * np.exp(-1/abs(test_4['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.513401905044875"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(target.smoothed, data_4.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_4.fpred.to_csv(\"submission_rf_pred_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb_2 = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:37:10] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train_4, y_train.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb_pred_train = xgb.predict(X_train_4)\n",
    "xgb_pred_val = xgb.predict(X_val_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_4[\"xgb_pred\"] = xgb_pred_train\n",
    "X_val_4[\"xgb_pred\"] = xgb_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35509580747915026\n",
      "0.5821345964262897\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(X_train_4.xgb_pred, y_train.smoothed))\n",
    "print(mean_absolute_error(X_val_4.xgb_pred, y_val.smoothed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:47:23] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_2.fit(data_4, target.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb_pred_full_train = xgb_2.predict(data_4)\n",
    "xgb_pred_test = xgb_2.predict(test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_4[\"xgb_pred\"] = xgb_pred_full_train\n",
    "data_4[\"xgb_fpred\"] = np.sign(data_4['xgb_pred'])* np.exp(-1/abs(data_4['xgb_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_4[\"xgb_pred\"] = xgb_pred_test\n",
    "test_4[\"xgb_fpred\"] = np.sign(test_4['xgb_pred'])* np.exp(-1/abs(test_4['xgb_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.368835986301781\n",
      "0.5425651188313596\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(data_4.xgb_pred, target.smoothed))\n",
    "print(mean_absolute_error(data_4.xgb_fpred, target.smoothed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_4.xgb_pred.to_csv(\"submissions/submission_xgb_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_4.xgb_fpred.to_csv(\"submissions/submission_xgb_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pipeline and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('selector', SelectKBest(f_regression)),\n",
    "        ('model', xgb)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params_xgb = {'selector__k':[10,20,30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "search = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = params_xgb,\n",
    "    scoring = 'neg_mean_absolute_error',\n",
    "    cv = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:52:23] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:32] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:37] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:39] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('selector',\n",
       "                                        SelectKBest(k=10,\n",
       "                                                    score_func=<function f_regression at 0x1a1e4c2950>)),\n",
       "                                       ('model',\n",
       "                                        XGBRegressor(base_score=0.5,\n",
       "                                                     booster='gbtree',\n",
       "                                                     colsample_bylevel=1,\n",
       "                                                     colsample_bynode=1,\n",
       "                                                     colsample_bytree=1,\n",
       "                                                     gamma=0,\n",
       "                                                     importance_type='gain',\n",
       "                                                     learning_rate=0.1,\n",
       "                                                     max_delta_step=0,\n",
       "                                                     max_depth=3,\n",
       "                                                     min_child...\n",
       "                                                     n_estimators=100, n_jobs=1,\n",
       "                                                     nthread=None,\n",
       "                                                     objective='reg:linear',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=None, silent=None,\n",
       "                                                     subsample=1,\n",
       "                                                     verbosity=1))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'selector__k': [10, 20, 30]}, pre_dispatch='2*n_jobs',\n",
       "             refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train_4, y_train.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selector__k': 30}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.28220438293246847"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.268365924996182\n",
      "0.6125804086151287\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(search.predict(X_train_4), y_train.smoothed))\n",
    "print(mean_absolute_error(search.predict(X_val_4), y_val.smoothed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LSTM Univariate model / xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_I1 = X_train.loc[:,\"I_1_lag_20\":\"I_1_lag_0\"]\n",
    "X_train_I2 = X_train.loc[:,\"I_2_lag_20\":\"I_2_lag_0\"]\n",
    "X_train_I3 = X_train.loc[:,\"I_3_lag_20\":\"I_3_lag_0\"]\n",
    "X_train_I4 = X_train.loc[:,\"I_4_lag_20\":\"I_4_lag_0\"]\n",
    "X_train_I5 = X_train.loc[:,\"I_5_lag_20\":\"I_5_lag_0\"]\n",
    "X_train_I6 = X_train.loc[:,\"I_6_lag_20\":\"I_6_lag_0\"]\n",
    "X_train_I7 = X_train.loc[:,\"I_7_lag_20\":\"I_7_lag_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_val_I1 = X_val.loc[:,\"I_1_lag_20\":\"I_1_lag_0\"]\n",
    "X_val_I2 = X_val.loc[:,\"I_2_lag_20\":\"I_2_lag_0\"]\n",
    "X_val_I3 = X_val.loc[:,\"I_3_lag_20\":\"I_3_lag_0\"]\n",
    "X_val_I4 = X_val.loc[:,\"I_4_lag_20\":\"I_4_lag_0\"]\n",
    "X_val_I5 = X_val.loc[:,\"I_5_lag_20\":\"I_5_lag_0\"]\n",
    "X_val_I6 = X_val.loc[:,\"I_6_lag_20\":\"I_6_lag_0\"]\n",
    "X_val_I7 = X_val.loc[:,\"I_7_lag_20\":\"I_7_lag_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test_I1 = test.loc[:,\"I_1_lag_20\":\"I_1_lag_0\"]\n",
    "X_test_I2 = test.loc[:,\"I_2_lag_20\":\"I_2_lag_0\"]\n",
    "X_test_I3 = test.loc[:,\"I_3_lag_20\":\"I_3_lag_0\"]\n",
    "X_test_I4 = test.loc[:,\"I_4_lag_20\":\"I_4_lag_0\"]\n",
    "X_test_I5 = test.loc[:,\"I_5_lag_20\":\"I_5_lag_0\"]\n",
    "X_test_I6 = test.loc[:,\"I_6_lag_20\":\"I_6_lag_0\"]\n",
    "X_test_I7 = test.loc[:,\"I_7_lag_20\":\"I_7_lag_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_I1 = sc_x.fit_transform(X_train_I1)\n",
    "X_train_I2 = sc_x.fit_transform(X_train_I2)\n",
    "X_train_I3 = sc_x.fit_transform(X_train_I3)\n",
    "X_train_I4 = sc_x.fit_transform(X_train_I4)\n",
    "X_train_I5 = sc_x.fit_transform(X_train_I5)\n",
    "X_train_I6 = sc_x.fit_transform(X_train_I6)\n",
    "X_train_I7 = sc_x.fit_transform(X_train_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_val_I1 = sc_x.transform(X_val_I1)\n",
    "X_val_I2 = sc_x.transform(X_val_I2)\n",
    "X_val_I3 = sc_x.transform(X_val_I3)\n",
    "X_val_I4 = sc_x.transform(X_val_I4)\n",
    "X_val_I5 = sc_x.transform(X_val_I5)\n",
    "X_val_I6 = sc_x.transform(X_val_I6)\n",
    "X_val_I7 = sc_x.transform(X_val_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test_I1 = sc_x.transform(X_test_I1)\n",
    "X_test_I2 = sc_x.transform(X_test_I2)\n",
    "X_test_I3 = sc_x.transform(X_test_I3)\n",
    "X_test_I4 = sc_x.transform(X_test_I4)\n",
    "X_test_I5 = sc_x.transform(X_test_I5)\n",
    "X_test_I6 = sc_x.transform(X_test_I6)\n",
    "X_test_I7 = sc_x.transform(X_test_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_I1 = np.expand_dims(X_train_I1, axis=-1)\n",
    "X_train_I2 = np.expand_dims(X_train_I2, axis=-1)\n",
    "X_train_I3 = np.expand_dims(X_train_I3, axis=-1)\n",
    "X_train_I4 = np.expand_dims(X_train_I4, axis=-1)\n",
    "X_train_I5 = np.expand_dims(X_train_I5, axis=-1)\n",
    "X_train_I6 = np.expand_dims(X_train_I6, axis=-1)\n",
    "X_train_I7 = np.expand_dims(X_train_I7, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_val_I1 = np.expand_dims(X_val_I1, axis=-1)\n",
    "X_val_I2 = np.expand_dims(X_val_I2, axis=-1)\n",
    "X_val_I3 = np.expand_dims(X_val_I3, axis=-1)\n",
    "X_val_I4 = np.expand_dims(X_val_I4, axis=-1)\n",
    "X_val_I5 = np.expand_dims(X_val_I5, axis=-1)\n",
    "X_val_I6 = np.expand_dims(X_val_I6, axis=-1)\n",
    "X_val_I7 = np.expand_dims(X_val_I7, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test_I1 = np.expand_dims(X_test_I1, axis=-1)\n",
    "X_test_I2 = np.expand_dims(X_test_I2, axis=-1)\n",
    "X_test_I3 = np.expand_dims(X_test_I3, axis=-1)\n",
    "X_test_I4 = np.expand_dims(X_test_I4, axis=-1)\n",
    "X_test_I5 = np.expand_dims(X_test_I5, axis=-1)\n",
    "X_test_I6 = np.expand_dims(X_test_I6, axis=-1)\n",
    "X_test_I7 = np.expand_dims(X_test_I7, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 158\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_univariate_1 = tf.data.Dataset.from_tensor_slices((X_train_I1, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_2 = tf.data.Dataset.from_tensor_slices((X_train_I2, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_3 = tf.data.Dataset.from_tensor_slices((X_train_I3, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_4 = tf.data.Dataset.from_tensor_slices((X_train_I4, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_5 = tf.data.Dataset.from_tensor_slices((X_train_I5, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_6 = tf.data.Dataset.from_tensor_slices((X_train_I6, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_7 = tf.data.Dataset.from_tensor_slices((X_train_I7, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_univariate_1 = tf.data.Dataset.from_tensor_slices((X_val_I1, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_2 = tf.data.Dataset.from_tensor_slices((X_val_I2, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_3 = tf.data.Dataset.from_tensor_slices((X_val_I3, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_4 = tf.data.Dataset.from_tensor_slices((X_val_I4, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_5 = tf.data.Dataset.from_tensor_slices((X_val_I5, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_6 = tf.data.Dataset.from_tensor_slices((X_val_I6, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_7 = tf.data.Dataset.from_tensor_slices((X_val_I7, y_val.smoothed)).batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(val_univariate_1.take(1))\n",
    "y_train.smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def R2(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simple_lstm_model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_5 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_6 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_7 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simple_lstm_model_1.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_2.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_3.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_4.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_5.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_6.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_7.compile(optimizer='adam', loss='mae', metrics=[R2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, _ in val_univariate.take(1):\n",
    "    print(simple_lstm_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 10\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history_1 = simple_lstm_model_1.fit(train_univariate_1, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_1, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_2 = simple_lstm_model_2.fit(train_univariate_2, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_2, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_3 = simple_lstm_model_3.fit(train_univariate_3, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_3, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_4 = simple_lstm_model_4.fit(train_univariate_4, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_4, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_5 = simple_lstm_model_5.fit(train_univariate_5, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_5, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_6 = simple_lstm_model_6.fit(train_univariate_6, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_6, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_7 = simple_lstm_model_7.fit(train_univariate_7, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_7, validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_2.history[\"loss\"], color=\"b\", label=\"Training Loss\")\n",
    "plt.plot(history_2.history[\"val_loss\"], color=\"r\", label=\"Validation Loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"R2\"], color=\"b\", label = \"Training R2\")\n",
    "plt.plot(history.history[\"val_R2\"], color=\"r\", label =\"Validation R2\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# prediction on train values\n",
    "lstm_pred_I1 = simple_lstm_model_1.predict(X_train_I1)\n",
    "lstm_pred_I2 = simple_lstm_model_2.predict(X_train_I2)\n",
    "lstm_pred_I3 = simple_lstm_model_3.predict(X_train_I3)\n",
    "lstm_pred_I4 = simple_lstm_model_4.predict(X_train_I4)\n",
    "lstm_pred_I5 = simple_lstm_model_5.predict(X_train_I5)\n",
    "lstm_pred_I6 = simple_lstm_model_6.predict(X_train_I6)\n",
    "lstm_pred_I7 = simple_lstm_model_7.predict(X_train_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# prediction on validation values\n",
    "lstm_val_pred_I1 = simple_lstm_model_1.predict(X_val_I1)\n",
    "lstm_val_pred_I2 = simple_lstm_model_2.predict(X_val_I2)\n",
    "lstm_val_pred_I3 = simple_lstm_model_3.predict(X_val_I3)\n",
    "lstm_val_pred_I4 = simple_lstm_model_4.predict(X_val_I4)\n",
    "lstm_val_pred_I5 = simple_lstm_model_5.predict(X_val_I5)\n",
    "lstm_val_pred_I6 = simple_lstm_model_6.predict(X_val_I6)\n",
    "lstm_val_pred_I7 = simple_lstm_model_7.predict(X_val_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# prediction on test values\n",
    "lstm_test_pred_I1 = simple_lstm_model_1.predict(X_test_I1)\n",
    "lstm_test_pred_I2 = simple_lstm_model_2.predict(X_test_I2)\n",
    "lstm_test_pred_I3 = simple_lstm_model_3.predict(X_test_I3)\n",
    "lstm_test_pred_I4 = simple_lstm_model_4.predict(X_test_I4)\n",
    "lstm_test_pred_I5 = simple_lstm_model_5.predict(X_test_I5)\n",
    "lstm_test_pred_I6 = simple_lstm_model_6.predict(X_test_I6)\n",
    "lstm_test_pred_I7 = simple_lstm_model_7.predict(X_test_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_train = {\"lstm_pred_I1\":lstm_pred_I1[:,0], \"lstm_pred_I2\":lstm_pred_I2[:,0], \"lstm_pred_I3\":lstm_pred_I3[:,0],\n",
    "    \"lstm_pred_I4\":lstm_pred_I4[:,0], \"lstm_pred_I5\":lstm_pred_I5[:,0], \"lstm_pred_I6\":lstm_pred_I6[:,0], \n",
    "     \"lstm_pred_I7\":lstm_pred_I7[:,0]}\n",
    "lstm_I_train = pd.DataFrame(data=d_train, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm_wI_train = pd.concat([X_train.iloc[:,0:7], lstm_I_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_val = {\"lstm_pred_I1\":lstm_val_pred_I1[:,0], \"lstm_pred_I2\":lstm_val_pred_I2[:,0], \"lstm_pred_I3\":lstm_val_pred_I3[:,0],\n",
    "    \"lstm_pred_I4\":lstm_val_pred_I4[:,0], \"lstm_pred_I5\":lstm_val_pred_I5[:,0], \"lstm_pred_I6\":lstm_val_pred_I6[:,0], \n",
    "     \"lstm_pred_I7\":lstm_val_pred_I7[:,0]}\n",
    "lstm_I_val = pd.DataFrame(data=d_val, index=X_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm_wI_val = pd.concat([X_val.iloc[:,0:7], lstm_I_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm_wI_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_test = {\"lstm_pred_I1\":lstm_test_pred_I1[:,0], \"lstm_pred_I2\":lstm_test_pred_I2[:,0], \"lstm_pred_I3\":lstm_test_pred_I3[:,0],\n",
    "    \"lstm_pred_I4\":lstm_test_pred_I4[:,0], \"lstm_pred_I5\":lstm_test_pred_I5[:,0], \"lstm_pred_I6\":lstm_test_pred_I6[:,0], \n",
    "     \"lstm_pred_I7\":lstm_test_pred_I7[:,0]}\n",
    "lstm_I_test = pd.DataFrame(data=d_test, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm_wI_test = pd.concat([test.iloc[:,0:7], lstm_I_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's train an adaboost on the lstm predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adaboost = AdaBoostRegressor()\n",
    "adaboost.fit(lstm_wI_train, y_train.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ada_train_pred = adaboost.predict(lstm_wI_train)\n",
    "ada_val_pred = adaboost.predict(lstm_wI_val)\n",
    "ada_test_pred = adaboost.predict(lstm_wI_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm_wI_train[\"pred\"] = ada_train_pred\n",
    "lstm_wI_val[\"pred\"] = ada_val_pred\n",
    "lstm_wI_test[\"pred\"] = ada_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm_wI_train[\"fpred\"] = np.sign(lstm_wI_train[\"pred\"])*np.exp(-1/abs(lstm_wI_train[\"pred\"]))\n",
    "lstm_wI_val[\"fpred\"] = np.sign(lstm_wI_val[\"pred\"])*np.exp(-1/abs(lstm_wI_val[\"pred\"]))\n",
    "lstm_wI_test[\"fpred\"] = np.sign(lstm_wI_test[\"pred\"])*np.exp(-1/abs(lstm_wI_test[\"pred\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(mean_absolute_error(y_train.smoothed,lstm_wI_train[\"fpred\"]))\n",
    "print(mean_absolute_error(y_val.smoothed,lstm_wI_val[\"fpred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Submission of the lstm/adaboost model on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm_wI_test.fpred.to_csv(\"submission_lstm_adaboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop([\"target\"],axis=1)\n",
    "y_train = df_train.loc[:,[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_val = df_val.drop([\"target\"],axis=1)\n",
    "y_val = df_val.loc[:,[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Daily Rate of Change for each strategy \n",
    "def I_roc(dataset):\n",
    "    I_roc_list = []\n",
    "    for i in range(1,8):\n",
    "        for j in range(20): \n",
    "            dataset[\"I_{}_roc_{}\".format(i,j)] = np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                                  format(i,j)]/dataset[\"I_{}_lag_{}\".format(i,j+1)])\n",
    "            I_roc_list.append(\"I_{}_roc_{}\".format(i,j))\n",
    "    return I_roc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_roc_list = I_roc(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Monthly return for each strategy\n",
    "def I_roc20(dataset):\n",
    "    I_roc20_list = []\n",
    "    for i in range(1,8):\n",
    "        dataset[\"I_{}_roc20\".format(i)] = 0\n",
    "        I_roc20_list.append(\"I_{}_roc20\".format(i))\n",
    "        for j in range(20):\n",
    "            dataset[\"I_{}_roc20\".format(i)] += np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                                  format(i,j)]/dataset[\"I_{}_lag_{}\".format(i,j+1)])\n",
    "    return I_roc20_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_roc20_list = I_roc20(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# last week return for each strategy\n",
    "def I_roc5(dataset):\n",
    "    I_roc5_list = []\n",
    "    for i in range(1,8):\n",
    "        dataset[\"I_{}_roc5\".format(i)] = 0\n",
    "        I_roc5_list.append(\"I_{}_roc5\".format(i))\n",
    "        for j in range(5):\n",
    "            dataset[\"I_{}_roc5\".format(i)] += np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                                  format(i,j)]/dataset[\"I_{}_lag_{}\".format(i,j+1)])\n",
    "    return I_roc5_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_roc5_list = I_roc5(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Daily Rate of Change for each macro-economic feature \n",
    "def X_roc(dataset):    \n",
    "    X_roc_list = []\n",
    "    for i in range(1,4):\n",
    "        for j in range(20):\n",
    "            dataset[\"X_{}_roc_{}\".format(i,j)] = np.log(dataset[\"X_{}_lag_{}\".\n",
    "                                                                  format(i,j)]/dataset[\"X_{}_lag_{}\".format(i,j+1)])\n",
    "            X_roc_list.append(\"X_{}_roc_{}\".format(i,j))\n",
    "    return X_roc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_roc_list = X_roc(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Monthly return for each macro-economic feature\n",
    "def X_roc20(dataset):\n",
    "    X_roc20_list = []\n",
    "    for i in range(1,4):\n",
    "        dataset[\"X_{}_roc20\".format(i)] = 0\n",
    "        X_roc20_list.append(\"X_{}_roc20\".format(i))\n",
    "        for j in range(20):\n",
    "            dataset[\"X_{}_roc20\".format(i)] += np.log(dataset[\"X_{}_lag_{}\".\n",
    "                                                              format(i,j)]/dataset[\"X_{}_lag_{}\".format(i,j+1)])\n",
    "    return X_roc20_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_roc20_list = X_roc20(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# last week return for each macro-economic feature\n",
    "def X_roc5(dataset):\n",
    "    X_roc5_list = []\n",
    "    for i in range(1,4):\n",
    "        dataset[\"X_{}_roc5\".format(i)] = 0\n",
    "        X_roc5_list.append(\"X_{}_roc5\".format(i))\n",
    "        for j in range(5):\n",
    "            dataset[\"X_{}_roc5\".format(i)] += np.log(dataset[\"X_{}_lag_{}\".\n",
    "                                                                  format(i,j)]/dataset[\"X_{}_lag_{}\".format(i,j+1)])\n",
    "    return X_roc5_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_roc5_list = X_roc5(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Weekly rate of change shifted with a window step = 5 \n",
    "def I_roc5_shifted(dataset):\n",
    "    I_roc5_shifted_list = []\n",
    "    for s in range(1,4):\n",
    "        for i in range(1,8):\n",
    "            dataset[\"I_{}_roc5_S{}\".format(i,s)] = 0\n",
    "            I_roc5_shifted_list.append(\"I_{}_roc5_S{}\".format(i,s))\n",
    "            for j in range(5):\n",
    "                dataset[\"I_{}_roc5_S{}\".format(i,s)] += np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                     format(i,j+(5*s))]/dataset[\"I_{}_lag_{}\".format(i,j+(5*s)+1)])\n",
    "    return I_roc5_shifted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_roc5_shifted_list = I_roc5_shifted(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# last week weighted return \n",
    "def I_wr(dataset):\n",
    "    dataset[\"I_wr\"] = 0\n",
    "    for i in range(1,8):\n",
    "        for j in range(5):\n",
    "            dataset[\"I_wr\"] += (np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                         format(i,j)]/dataset[\"I_{}_lag_{}\".format(i,j+1)])\n",
    "            * dataset[\"weight_I_{}\".format(i)]) * 252/5\n",
    "    return [\"I_wr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_wr_list = I_wr(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Weighted rate of return for each shifted window\n",
    "# This function need \"I_roc5_shifted\" appended to the dataset first\n",
    "def I_wr_shifted(dataset):\n",
    "    I_wr_shifted_list = []\n",
    "    for s in range(1,4):\n",
    "        dataset[\"I_wr_S{}\".format(s)] = 0\n",
    "        I_wr_shifted_list.append(\"I_wr_S{}\".format(s))\n",
    "        for i in range(1,8):\n",
    "            dataset[\"I_wr_S{}\".format(s)] += (dataset[\"I_{}_roc5_S{}\".format(i,s)] * \n",
    "                                              dataset[\"weight_I_{}\".format(i)] * (252/5))\n",
    "    return I_wr_shifted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_wr_shifted_list = I_wr_shifted(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cov of strategies I\n",
    "# This function need \"I_roc\" and \"I_roc20\" appended to the dataset first  \n",
    "def I_cov(dataset):\n",
    "    I_cov_list = []\n",
    "    for i in range(1,8):\n",
    "        for j in range(1,8):\n",
    "            dataset[\"I_cov_{}{}\".format(i,j)] = 0\n",
    "            I_cov_list.append(\"I_cov_{}{}\".format(i,j))\n",
    "            for t in range(20):\n",
    "                dataset[\"I_cov_{}{}\".format(i,j)] += ((dataset[\"I_{}_roc_{}\".format(i,t)]\n",
    "                                                      - dataset[\"I_{}_roc20\".format(i)])\n",
    "                                                     * (dataset[\"I_{}_roc_{}\".format(j,t)] \n",
    "                                                     - dataset[\"I_{}_roc20\".format(j)]))\n",
    "    return I_cov_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_cov_list = I_cov(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Cov of X \n",
    "# This function need \"X_roc\" and \"X_roc20\" appended to the dataset first  \n",
    "def X_cov(dataset):\n",
    "    X_cov_list = []\n",
    "    for i in range(1,4):\n",
    "        for j in range(1,4):\n",
    "            dataset[\"X_cov_{}{}\".format(i,j)] = 0\n",
    "            X_cov_list.append(\"X_cov_{}{}\".format(i,j))\n",
    "            for t in range(20):\n",
    "                dataset[\"X_cov_{}{}\".format(i,j)] += ((dataset[\"X_{}_roc_{}\".format(i,t)]\n",
    "                                                      - dataset[\"X_{}_roc20\".format(i)])\n",
    "                                                    * (dataset[\"X_{}_roc_{}\".format(j,t)] \n",
    "                                                    - dataset[\"X_{}_roc20\".format(j)]))\n",
    "    return X_cov_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_cov_list = X_cov(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Volatility of blended strategies\n",
    "# This function need \"I_cov\" appended to the dataset first\n",
    "def sigma(dataset):\n",
    "    dataset[\"sigma\"] = 0\n",
    "    for i in range(1,8):\n",
    "        for j in range(1,8):\n",
    "            dataset[\"sigma\"] += (252*dataset['weight_I_{}'.format(i)]*dataset['weight_I_{}'.format(j)]\n",
    "                                * dataset[\"I_cov_{}{}\".format(i,j)])\n",
    "    dataset[\"sigma\"] = np.sqrt(dataset[\"sigma\"])\n",
    "    dataset[\"sigma\"][dataset[\"sigma\"]<0.005] = 0.005\n",
    "    return [\"sigma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "sigma_list = sigma(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Sharpe ratio of the portfolio (blended strategies)\n",
    "# This function need \"sigma\" and \"I_wr\" appended to the dataset first\n",
    "def SR(dataset):\n",
    "    dataset[\"SR\"] = dataset[\"I_wr\"]/dataset[\"sigma\"]\n",
    "    return [\"SR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "SR_list = SR(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Shifted Sharpe ratios of the portfolio \n",
    "# This function need \"sigma\" and \"I_wr_shifted\" appended to the dataset first \n",
    "def SR_shifted(dataset):\n",
    "    SR_shifted_list = []\n",
    "    for s in range(1,4):\n",
    "        dataset[\"SR_S{}\".format(s)] = dataset[\"I_wr_S{}\".format(s)]/dataset[\"sigma\"]\n",
    "        SR_shifted_list.append(\"SR_S{}\".format(s))\n",
    "    return SR_shifted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "SR_shifted_list = SR_shifted(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Sharpe raio of each strategy alone\n",
    "# This function need \"I_cov\" and \"I_roc5\" appended to the dataset first \n",
    "def SR_I(dataset):\n",
    "    SR_I_list = []\n",
    "    for i in range(1,8):\n",
    "        dataset[\"SR_I{}\".format(i)] = np.sqrt(dataset[\"I_cov_{}{}\".format(i,i)])\n",
    "        dataset[\"SR_I{}\".format(i)][dataset[\"SR_I{}\".format(i)]<0.005] = 0.005 \n",
    "        dataset[\"SR_I{}\".format(i)] = dataset[\"I_{}_roc5\".format(i)] / dataset[\"SR_I{}\".format(i)]\n",
    "        SR_I_list.append(\"SR_I{}\".format(i))\n",
    "    return SR_I_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "SR_I_list = SR_I(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop([\"target\"],axis=1)\n",
    "y_train = df_train.loc[:,[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val.drop([\"target\"],axis=1)\n",
    "y_val = df_val.loc[:,[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of indicators operations to append to the dataset\n",
    "indicators = [I_roc, I_roc20, I_roc5, I_roc5_shifted, I_cov, SR_I, X_roc, X_roc20, X_roc5, sigma, I_wr_shifted, SR_shifted, X_cov, I_wr, SR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def engineering(dataset, indicators):\n",
    "    ''' Append the indicators to the dataset \n",
    "    '''\n",
    "    for indicator in indicators:\n",
    "        indicator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineering(X_train, indicators)\n",
    "engineering(X_val, indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineering(data, indicators)\n",
    "engineering(test, indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of indicators list to keep\n",
    "selection = SR_I_list+I_roc5_list+X_roc5_list+I_cov_list+X_cov_list+SR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4 = X_train[selection]\n",
    "X_val_4 = X_val[selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4 = data[selection]\n",
    "test_4 = test[selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
