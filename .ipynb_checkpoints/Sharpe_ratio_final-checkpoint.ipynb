{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Prediction of Sharpe ratio for blends of quantitative strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "from joblib import load\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/Training_Input_2dx8C9Q.csv\")\n",
    "y = pd.read_csv(\"Data/Training_Output_IJhBXtA.csv\")\n",
    "test = pd.read_csv(\"Data/Testing_Input_dPKY3Rf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 218)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>weight_I_1</th>\n",
       "      <th>weight_I_2</th>\n",
       "      <th>weight_I_3</th>\n",
       "      <th>weight_I_4</th>\n",
       "      <th>weight_I_5</th>\n",
       "      <th>weight_I_6</th>\n",
       "      <th>weight_I_7</th>\n",
       "      <th>I_1_lag_20</th>\n",
       "      <th>I_1_lag_19</th>\n",
       "      <th>...</th>\n",
       "      <th>X_3_lag_9</th>\n",
       "      <th>X_3_lag_8</th>\n",
       "      <th>X_3_lag_7</th>\n",
       "      <th>X_3_lag_6</th>\n",
       "      <th>X_3_lag_5</th>\n",
       "      <th>X_3_lag_4</th>\n",
       "      <th>X_3_lag_3</th>\n",
       "      <th>X_3_lag_2</th>\n",
       "      <th>X_3_lag_1</th>\n",
       "      <th>X_3_lag_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.047398</td>\n",
       "      <td>...</td>\n",
       "      <td>101.383783</td>\n",
       "      <td>102.054669</td>\n",
       "      <td>102.375596</td>\n",
       "      <td>103.148605</td>\n",
       "      <td>103.148605</td>\n",
       "      <td>103.046483</td>\n",
       "      <td>103.075701</td>\n",
       "      <td>103.134043</td>\n",
       "      <td>103.221509</td>\n",
       "      <td>103.338192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.912339</td>\n",
       "      <td>...</td>\n",
       "      <td>100.911142</td>\n",
       "      <td>100.938707</td>\n",
       "      <td>100.993926</td>\n",
       "      <td>101.132016</td>\n",
       "      <td>100.745489</td>\n",
       "      <td>100.524617</td>\n",
       "      <td>100.303743</td>\n",
       "      <td>100.276090</td>\n",
       "      <td>100.303743</td>\n",
       "      <td>100.554527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.481681</td>\n",
       "      <td>...</td>\n",
       "      <td>100.373084</td>\n",
       "      <td>100.581716</td>\n",
       "      <td>100.313489</td>\n",
       "      <td>100.790251</td>\n",
       "      <td>101.013756</td>\n",
       "      <td>100.686030</td>\n",
       "      <td>100.686030</td>\n",
       "      <td>100.060233</td>\n",
       "      <td>99.747384</td>\n",
       "      <td>99.970889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.124618</td>\n",
       "      <td>...</td>\n",
       "      <td>100.844136</td>\n",
       "      <td>101.040072</td>\n",
       "      <td>101.055122</td>\n",
       "      <td>101.567682</td>\n",
       "      <td>101.703322</td>\n",
       "      <td>101.974603</td>\n",
       "      <td>101.733422</td>\n",
       "      <td>101.838963</td>\n",
       "      <td>102.080144</td>\n",
       "      <td>101.688272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.665093</td>\n",
       "      <td>99.482389</td>\n",
       "      <td>99.604192</td>\n",
       "      <td>100.030499</td>\n",
       "      <td>99.847797</td>\n",
       "      <td>100.426310</td>\n",
       "      <td>100.426310</td>\n",
       "      <td>100.822217</td>\n",
       "      <td>100.913521</td>\n",
       "      <td>100.852619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  weight_I_1  weight_I_2  weight_I_3  weight_I_4  weight_I_5  weight_I_6  \\\n",
       "0   0        0.15        0.00        0.05        0.80        0.00         0.0   \n",
       "1   1        0.00        0.00        0.00        0.40        0.25         0.0   \n",
       "2   2        0.85        0.00        0.00        0.15        0.00         0.0   \n",
       "3   3        0.00        0.00        0.70        0.05        0.25         0.0   \n",
       "4   4        0.00        0.55        0.05        0.00        0.00         0.0   \n",
       "\n",
       "   weight_I_7  I_1_lag_20  I_1_lag_19  ...   X_3_lag_9   X_3_lag_8  \\\n",
       "0        0.00       100.0  100.047398  ...  101.383783  102.054669   \n",
       "1        0.35       100.0   99.912339  ...  100.911142  100.938707   \n",
       "2        0.00       100.0   99.481681  ...  100.373084  100.581716   \n",
       "3        0.00       100.0  100.124618  ...  100.844136  101.040072   \n",
       "4        0.40       100.0  100.000000  ...   99.665093   99.482389   \n",
       "\n",
       "    X_3_lag_7   X_3_lag_6   X_3_lag_5   X_3_lag_4   X_3_lag_3   X_3_lag_2  \\\n",
       "0  102.375596  103.148605  103.148605  103.046483  103.075701  103.134043   \n",
       "1  100.993926  101.132016  100.745489  100.524617  100.303743  100.276090   \n",
       "2  100.313489  100.790251  101.013756  100.686030  100.686030  100.060233   \n",
       "3  101.055122  101.567682  101.703322  101.974603  101.733422  101.838963   \n",
       "4   99.604192  100.030499   99.847797  100.426310  100.426310  100.822217   \n",
       "\n",
       "    X_3_lag_1   X_3_lag_0  \n",
       "0  103.221509  103.338192  \n",
       "1  100.303743  100.554527  \n",
       "2   99.747384   99.970889  \n",
       "3  102.080144  101.688272  \n",
       "4  100.913521  100.852619  \n",
       "\n",
       "[5 rows x 218 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-12.007941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.294867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.652308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.412364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8.517471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Target\n",
       "0   0 -12.007941\n",
       "1   1   2.294867\n",
       "2   2   0.652308\n",
       "3   3   2.412364\n",
       "4   4   8.517471"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4450, 218)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data = data.set_index(\"ID\")\n",
    "y = y.set_index(\"ID\")\n",
    "test = test.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 0 to 9999\n",
      "Columns: 217 entries, weight_I_1 to X_3_lag_0\n",
      "dtypes: float64(217)\n",
      "memory usage: 16.6 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 0 to 9999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Target  10000 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 156.2 KB\n"
     ]
    }
   ],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n"
     ]
    }
   ],
   "source": [
    "# Missing values ?\n",
    "print(True in data.isna(), True in y.isna(), True in test.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight_I_1',\n",
       " 'weight_I_2',\n",
       " 'weight_I_3',\n",
       " 'weight_I_4',\n",
       " 'weight_I_5',\n",
       " 'weight_I_6',\n",
       " 'weight_I_7',\n",
       " 'I_1_lag_20',\n",
       " 'I_1_lag_19',\n",
       " 'I_1_lag_18',\n",
       " 'I_1_lag_17',\n",
       " 'I_1_lag_16',\n",
       " 'I_1_lag_15',\n",
       " 'I_1_lag_14',\n",
       " 'I_1_lag_13',\n",
       " 'I_1_lag_12',\n",
       " 'I_1_lag_11',\n",
       " 'I_1_lag_10',\n",
       " 'I_1_lag_9',\n",
       " 'I_1_lag_8',\n",
       " 'I_1_lag_7',\n",
       " 'I_1_lag_6',\n",
       " 'I_1_lag_5',\n",
       " 'I_1_lag_4',\n",
       " 'I_1_lag_3',\n",
       " 'I_1_lag_2',\n",
       " 'I_1_lag_1',\n",
       " 'I_1_lag_0',\n",
       " 'I_2_lag_20',\n",
       " 'I_2_lag_19',\n",
       " 'I_2_lag_18',\n",
       " 'I_2_lag_17',\n",
       " 'I_2_lag_16',\n",
       " 'I_2_lag_15',\n",
       " 'I_2_lag_14',\n",
       " 'I_2_lag_13',\n",
       " 'I_2_lag_12',\n",
       " 'I_2_lag_11',\n",
       " 'I_2_lag_10',\n",
       " 'I_2_lag_9',\n",
       " 'I_2_lag_8',\n",
       " 'I_2_lag_7',\n",
       " 'I_2_lag_6',\n",
       " 'I_2_lag_5',\n",
       " 'I_2_lag_4',\n",
       " 'I_2_lag_3',\n",
       " 'I_2_lag_2',\n",
       " 'I_2_lag_1',\n",
       " 'I_2_lag_0',\n",
       " 'I_3_lag_20',\n",
       " 'I_3_lag_19',\n",
       " 'I_3_lag_18',\n",
       " 'I_3_lag_17',\n",
       " 'I_3_lag_16',\n",
       " 'I_3_lag_15',\n",
       " 'I_3_lag_14',\n",
       " 'I_3_lag_13',\n",
       " 'I_3_lag_12',\n",
       " 'I_3_lag_11',\n",
       " 'I_3_lag_10',\n",
       " 'I_3_lag_9',\n",
       " 'I_3_lag_8',\n",
       " 'I_3_lag_7',\n",
       " 'I_3_lag_6',\n",
       " 'I_3_lag_5',\n",
       " 'I_3_lag_4',\n",
       " 'I_3_lag_3',\n",
       " 'I_3_lag_2',\n",
       " 'I_3_lag_1',\n",
       " 'I_3_lag_0',\n",
       " 'I_4_lag_20',\n",
       " 'I_4_lag_19',\n",
       " 'I_4_lag_18',\n",
       " 'I_4_lag_17',\n",
       " 'I_4_lag_16',\n",
       " 'I_4_lag_15',\n",
       " 'I_4_lag_14',\n",
       " 'I_4_lag_13',\n",
       " 'I_4_lag_12',\n",
       " 'I_4_lag_11',\n",
       " 'I_4_lag_10',\n",
       " 'I_4_lag_9',\n",
       " 'I_4_lag_8',\n",
       " 'I_4_lag_7',\n",
       " 'I_4_lag_6',\n",
       " 'I_4_lag_5',\n",
       " 'I_4_lag_4',\n",
       " 'I_4_lag_3',\n",
       " 'I_4_lag_2',\n",
       " 'I_4_lag_1',\n",
       " 'I_4_lag_0',\n",
       " 'I_5_lag_20',\n",
       " 'I_5_lag_19',\n",
       " 'I_5_lag_18',\n",
       " 'I_5_lag_17',\n",
       " 'I_5_lag_16',\n",
       " 'I_5_lag_15',\n",
       " 'I_5_lag_14',\n",
       " 'I_5_lag_13',\n",
       " 'I_5_lag_12',\n",
       " 'I_5_lag_11',\n",
       " 'I_5_lag_10',\n",
       " 'I_5_lag_9',\n",
       " 'I_5_lag_8',\n",
       " 'I_5_lag_7',\n",
       " 'I_5_lag_6',\n",
       " 'I_5_lag_5',\n",
       " 'I_5_lag_4',\n",
       " 'I_5_lag_3',\n",
       " 'I_5_lag_2',\n",
       " 'I_5_lag_1',\n",
       " 'I_5_lag_0',\n",
       " 'I_6_lag_20',\n",
       " 'I_6_lag_19',\n",
       " 'I_6_lag_18',\n",
       " 'I_6_lag_17',\n",
       " 'I_6_lag_16',\n",
       " 'I_6_lag_15',\n",
       " 'I_6_lag_14',\n",
       " 'I_6_lag_13',\n",
       " 'I_6_lag_12',\n",
       " 'I_6_lag_11',\n",
       " 'I_6_lag_10',\n",
       " 'I_6_lag_9',\n",
       " 'I_6_lag_8',\n",
       " 'I_6_lag_7',\n",
       " 'I_6_lag_6',\n",
       " 'I_6_lag_5',\n",
       " 'I_6_lag_4',\n",
       " 'I_6_lag_3',\n",
       " 'I_6_lag_2',\n",
       " 'I_6_lag_1',\n",
       " 'I_6_lag_0',\n",
       " 'I_7_lag_20',\n",
       " 'I_7_lag_19',\n",
       " 'I_7_lag_18',\n",
       " 'I_7_lag_17',\n",
       " 'I_7_lag_16',\n",
       " 'I_7_lag_15',\n",
       " 'I_7_lag_14',\n",
       " 'I_7_lag_13',\n",
       " 'I_7_lag_12',\n",
       " 'I_7_lag_11',\n",
       " 'I_7_lag_10',\n",
       " 'I_7_lag_9',\n",
       " 'I_7_lag_8',\n",
       " 'I_7_lag_7',\n",
       " 'I_7_lag_6',\n",
       " 'I_7_lag_5',\n",
       " 'I_7_lag_4',\n",
       " 'I_7_lag_3',\n",
       " 'I_7_lag_2',\n",
       " 'I_7_lag_1',\n",
       " 'I_7_lag_0',\n",
       " 'X_1_lag_20',\n",
       " 'X_1_lag_19',\n",
       " 'X_1_lag_18',\n",
       " 'X_1_lag_17',\n",
       " 'X_1_lag_16',\n",
       " 'X_1_lag_15',\n",
       " 'X_1_lag_14',\n",
       " 'X_1_lag_13',\n",
       " 'X_1_lag_12',\n",
       " 'X_1_lag_11',\n",
       " 'X_1_lag_10',\n",
       " 'X_1_lag_9',\n",
       " 'X_1_lag_8',\n",
       " 'X_1_lag_7',\n",
       " 'X_1_lag_6',\n",
       " 'X_1_lag_5',\n",
       " 'X_1_lag_4',\n",
       " 'X_1_lag_3',\n",
       " 'X_1_lag_2',\n",
       " 'X_1_lag_1',\n",
       " 'X_1_lag_0',\n",
       " 'X_2_lag_20',\n",
       " 'X_2_lag_19',\n",
       " 'X_2_lag_18',\n",
       " 'X_2_lag_17',\n",
       " 'X_2_lag_16',\n",
       " 'X_2_lag_15',\n",
       " 'X_2_lag_14',\n",
       " 'X_2_lag_13',\n",
       " 'X_2_lag_12',\n",
       " 'X_2_lag_11',\n",
       " 'X_2_lag_10',\n",
       " 'X_2_lag_9',\n",
       " 'X_2_lag_8',\n",
       " 'X_2_lag_7',\n",
       " 'X_2_lag_6',\n",
       " 'X_2_lag_5',\n",
       " 'X_2_lag_4',\n",
       " 'X_2_lag_3',\n",
       " 'X_2_lag_2',\n",
       " 'X_2_lag_1',\n",
       " 'X_2_lag_0',\n",
       " 'X_3_lag_20',\n",
       " 'X_3_lag_19',\n",
       " 'X_3_lag_18',\n",
       " 'X_3_lag_17',\n",
       " 'X_3_lag_16',\n",
       " 'X_3_lag_15',\n",
       " 'X_3_lag_14',\n",
       " 'X_3_lag_13',\n",
       " 'X_3_lag_12',\n",
       " 'X_3_lag_11',\n",
       " 'X_3_lag_10',\n",
       " 'X_3_lag_9',\n",
       " 'X_3_lag_8',\n",
       " 'X_3_lag_7',\n",
       " 'X_3_lag_6',\n",
       " 'X_3_lag_5',\n",
       " 'X_3_lag_4',\n",
       " 'X_3_lag_3',\n",
       " 'X_3_lag_2',\n",
       " 'X_3_lag_1',\n",
       " 'X_3_lag_0']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the columns to understand the construction of the dataset. \n",
    "# It begins with lag_20 and finishes with lag_0 for the 10 time series. \n",
    "# So lag_20 represent the fist day of a month set to a value of \"100\" of investment.\n",
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is up to 50 different samples (different weights) for the same time serie of 26 days \n",
    "# (21 days of training and 5 days of prediction)\n",
    "# given a train dataset of 10 000 samples, if there are exactly 50 differents, we should obtain :\n",
    "# 10 000/50 = 200 unique (values) time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 200\n",
      "1 200\n",
      "2 200\n",
      "3 200\n",
      "4 200\n",
      "5 200\n",
      "6 200\n",
      "7 200\n",
      "8 200\n",
      "9 200\n",
      "10 200\n",
      "11 200\n",
      "12 200\n",
      "13 200\n",
      "14 200\n",
      "15 200\n",
      "16 200\n",
      "17 200\n",
      "18 200\n",
      "19 193\n",
      "20 1\n"
     ]
    }
   ],
   "source": [
    "# I think any strategy I or macro feature X could allow to find the number of unique values,\n",
    "# because for 20 different variations over the 21 days, only the same variations can give a same final value\n",
    "# there is no place for hazard here. \n",
    "\n",
    "# For strategy I_1 (for example) we can find our same 200 time series starting at lag_7 (14th day of the month)\n",
    "for i in range(0,21):\n",
    "    print(i,data[\"I_2_lag_\"+str(i)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 89\n",
      "1 89\n",
      "2 89\n",
      "3 89\n",
      "4 89\n",
      "5 89\n",
      "6 89\n",
      "7 88\n",
      "8 87\n",
      "9 87\n",
      "10 86\n",
      "11 86\n",
      "12 84\n",
      "13 84\n",
      "14 83\n",
      "15 82\n",
      "16 80\n",
      "17 80\n",
      "18 79\n",
      "19 77\n",
      "20 1\n"
     ]
    }
   ],
   "source": [
    "# Let's check for the test dataset with I_7 (for example) \n",
    "for i in range(0,21):\n",
    "    print(i,test[\"I_7_lag_\"+str(i)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same, i find the 89 unique values expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The metric is a L1 norm with values smoothed by a fonction : f(x) = sig(x)*exp(-1/abs(x))\n",
    "# The models will be train with the smoothed values of y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "y[\"smoothed\"] = y.apply(lambda x: np.sign(x)*np.exp(-1/abs(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the customized cross_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# A difficulty here is that the different samples for a same time serie are shuffled into the dataset. \n",
    "# So I can not just randomly split the dataset into a train/val dataset. \n",
    "# I have to make sure to have different time series in train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 289\n",
      "1 289\n",
      "2 289\n",
      "3 289\n",
      "4 285\n",
      "5 282\n",
      "6 280\n",
      "7 278\n",
      "8 277\n",
      "9 270\n",
      "10 266\n",
      "11 266\n",
      "12 262\n",
      "13 257\n",
      "14 255\n",
      "15 253\n",
      "16 250\n",
      "17 248\n",
      "18 242\n",
      "19 233\n",
      "20 1\n"
     ]
    }
   ],
   "source": [
    "# First, let's check if there are same time series in train and test \n",
    "# We have 200 unique values for train, 89 for test, if indeed time series are different\n",
    "# we should obtain 289 unique values in the concatenation oh both. \n",
    "check = pd.concat([data, test])\n",
    "for i in range(0,21):\n",
    "    print(i,check[\"I_7_lag_\"+str(i)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now know train and test are not shuffled and correspond to different time series.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# First, I group the samples by values of I_1_lag_0 (for example) \n",
    "data_gb = data.groupby(\"I_1_lag_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Then, samples that correspond to the same time serie will have the same IDGroup\n",
    "for i, ind in enumerate(list(data_gb.groups.values())):\n",
    "    data.loc[ind,\"IDgroup\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "columns = data.columns.tolist()\n",
    "columns = columns[-1:]+columns[:-1]\n",
    "data = data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "id_group = data['IDgroup']\n",
    "X = data.drop(['IDgroup'], axis=1)\n",
    "gkf = GroupKFold(5)\n",
    "splits = gkf.split(data.loc[:, :], y.smoothed, id_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done ! \n",
    "# When I will run my models, i will split my different cross/val sets\n",
    "# by the GroupKFold function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recreate the benchmark score\n",
    "# The benchmark is the average Sharpe ratio of the training period\n",
    "# The challenge doesn't specify if the train period taken is the entire train period or \n",
    "# the train period after cross_val split. \n",
    "# I choose to take the entire train period. However, this is not really important for the results. \n",
    "\n",
    "avg = y.Target.mean()\n",
    "X_benchmark = X.copy()\n",
    "X_benchmark[\"pred_benchmark\"] = np.sign(avg)*np.exp(-1/abs(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Sharpe ratio over the training set is : 1.2883218600109478\n",
      "The Benchmark score is : 0.5949589309692296\n"
     ]
    }
   ],
   "source": [
    "benchmark_score = mean_absolute_error(y.smoothed, X_benchmark[\"pred_benchmark\"])\n",
    "print(\"The average Sharpe ratio over the training set is :\",avg)\n",
    "print(\"The Benchmark score is :\", benchmark_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "list_indicators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "list_selection = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Daily Rate of Change for each strategy \n",
    "def I_roc(dataset):\n",
    "    I_roc_list = []\n",
    "    for i in range(1,8):\n",
    "        for j in range(20): \n",
    "            dataset[\"I_{}_roc_{}\".format(i,j)] = np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                                  format(i,j)]/dataset[\"I_{}_lag_{}\".format(i,j+1)])\n",
    "            I_roc_list.append(\"I_{}_roc_{}\".format(i,j))\n",
    "    return I_roc_list\n",
    "list_indicators.append(I_roc)\n",
    "list_selection.append(\"I_roc_list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_roc_list = I_roc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Monthly return for each strategy\n",
    "def I_roc20(dataset):\n",
    "    I_roc20_list = []\n",
    "    for i in range(1,8):\n",
    "        dataset[\"I_{}_roc20\".format(i)] = 0\n",
    "        I_roc20_list.append(\"I_{}_roc20\".format(i))\n",
    "        for j in range(20):\n",
    "            dataset[\"I_{}_roc20\".format(i)] += np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                format(i,j)]/dataset[\"I_{}_lag_{}\".format(i,j+1)])\n",
    "    return I_roc20_list\n",
    "list_indicators.append(I_roc20)\n",
    "list_selection.append(\"I_roc20_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_roc20_list = I_roc20(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# last week return for each strategy\n",
    "def I_roc5(dataset):\n",
    "    I_roc5_list = []\n",
    "    for i in range(1,8):\n",
    "        dataset[\"I_{}_roc5\".format(i)] = 0\n",
    "        I_roc5_list.append(\"I_{}_roc5\".format(i))\n",
    "        for j in range(5):\n",
    "            dataset[\"I_{}_roc5\".format(i)] += np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                format(i,j)]/dataset[\"I_{}_lag_{}\".format(i,j+1)])\n",
    "\n",
    "    return I_roc5_list\n",
    "list_indicators.append(I_roc5)\n",
    "list_selection.append(\"I_roc5_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_roc5_list = I_roc5(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Daily Rate of Change for each macro-economic feature \n",
    "def X_roc(dataset):    \n",
    "    X_roc_list = []\n",
    "    for i in range(1,4):\n",
    "        for j in range(20):\n",
    "            dataset[\"X_{}_roc_{}\".format(i,j)] = np.log(dataset[\"X_{}_lag_{}\".\n",
    "                                                format(i,j)]/dataset[\"X_{}_lag_{}\".format(i,j+1)])\n",
    "            X_roc_list.append(\"X_{}_roc_{}\".format(i,j))\n",
    "    return X_roc_list\n",
    "list_indicators.append(X_roc)\n",
    "list_selection.append(\"X_roc_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_roc_list = X_roc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Monthly return for each macro-economic feature\n",
    "def X_roc20(dataset):\n",
    "    X_roc20_list = []\n",
    "    for i in range(1,4):\n",
    "        dataset[\"X_{}_roc20\".format(i)] = 0\n",
    "        X_roc20_list.append(\"X_{}_roc20\".format(i))\n",
    "        for j in range(20):\n",
    "            dataset[\"X_{}_roc20\".format(i)] += np.log(dataset[\"X_{}_lag_{}\".\n",
    "                                                    format(i,j)]/dataset[\"X_{}_lag_{}\".format(i,j+1)])\n",
    "    return X_roc20_list\n",
    "list_indicators.append(X_roc20)  \n",
    "list_selection.append(\"X_roc20_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_roc20_list = X_roc20(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# last week return for each macro-economic feature\n",
    "def X_roc5(dataset):\n",
    "    X_roc5_list = []\n",
    "    for i in range(1,4):\n",
    "        dataset[\"X_{}_roc5\".format(i)] = 0\n",
    "        X_roc5_list.append(\"X_{}_roc5\".format(i))\n",
    "        for j in range(5):\n",
    "            dataset[\"X_{}_roc5\".format(i)] += np.log(dataset[\"X_{}_lag_{}\".\n",
    "                                                format(i,j)]/dataset[\"X_{}_lag_{}\".format(i,j+1)])\n",
    "    return X_roc5_list\n",
    "list_indicators.append(X_roc5)\n",
    "list_selection.append(\"X_roc5_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_roc5_list = X_roc5(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Weekly rate of change shifted with a window step = 5 \n",
    "def I_roc5_shifted(dataset):\n",
    "    I_roc5_shifted_list = []\n",
    "    for s in range(1,4):\n",
    "        for i in range(1,8):\n",
    "            dataset[\"I_{}_roc5_S{}\".format(i,s)] = 0\n",
    "            I_roc5_shifted_list.append(\"I_{}_roc5_S{}\".format(i,s))\n",
    "            for j in range(5):\n",
    "                dataset[\"I_{}_roc5_S{}\".format(i,s)] += np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                    format(i,j+(5*s))]/dataset[\"I_{}_lag_{}\".format(i,j+(5*s)+1)])\n",
    "    return I_roc5_shifted_list\n",
    "list_indicators.append(I_roc5_shifted)\n",
    "list_selection.append(\"I_roc5_shifted_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_roc5_shifted_list = I_roc5_shifted(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# last week weighted return \n",
    "def I_wr(dataset):\n",
    "    dataset[\"I_wr\"] = 0\n",
    "    for i in range(1,8):\n",
    "        for j in range(5):\n",
    "            dataset[\"I_wr\"] += (np.log(dataset[\"I_{}_lag_{}\".\n",
    "                                                format(i,j)]/dataset[\"I_{}_lag_{}\".format(i,j+1)])\n",
    "            * dataset[\"weight_I_{}\".format(i)]) * 252/5\n",
    "    return [\"I_wr\"]\n",
    "list_indicators.append(I_wr)\n",
    "list_selection.append(\"I_wr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_wr_list = I_wr(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Weighted rate of return for each shifted window\n",
    "# This function need \"I_roc5_shifted\" appended to the dataset first\n",
    "def I_wr_shifted(dataset):\n",
    "    I_wr_shifted_list = []\n",
    "    for s in range(1,4):\n",
    "        dataset[\"I_wr_S{}\".format(s)] = 0\n",
    "        I_wr_shifted_list.append(\"I_wr_S{}\".format(s))\n",
    "        for i in range(1,8):\n",
    "            dataset[\"I_wr_S{}\".format(s)] += (dataset[\"I_{}_roc5_S{}\".format(i,s)] * \n",
    "                                              dataset[\"weight_I_{}\".format(i)] * (252/5))\n",
    "    return I_wr_shifted_list\n",
    "list_indicators.append(I_wr_shifted)\n",
    "list_selection.append(\"I_wr_shifted_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_wr_shifted_list = I_wr_shifted(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Covariances of strategies I\n",
    "# This function need \"I_roc\" and \"I_roc20\" appended to the dataset first  \n",
    "def I_cov(dataset):\n",
    "    I_cov_list = []\n",
    "    for i in range(1,8):\n",
    "        for j in range(1,8):\n",
    "            dataset[\"I_cov_{}{}\".format(i,j)] = 0\n",
    "            I_cov_list.append(\"I_cov_{}{}\".format(i,j))\n",
    "            for t in range(20):\n",
    "                dataset[\"I_cov_{}{}\".format(i,j)] += ((dataset[\"I_{}_roc_{}\".format(i,t)]\n",
    "                                                      - dataset[\"I_{}_roc20\".format(i)])\n",
    "                                                     * (dataset[\"I_{}_roc_{}\".format(j,t)] \n",
    "                                                     - dataset[\"I_{}_roc20\".format(j)]))\n",
    "    return I_cov_list\n",
    "list_indicators.append(I_cov)\n",
    "list_selection.append(\"I_cov_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "I_cov_list = I_cov(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Covariances of X \n",
    "# This function need \"X_roc\" and \"X_roc20\" appended to the dataset first  \n",
    "def X_cov(dataset):\n",
    "    X_cov_list = []\n",
    "    for i in range(1,4):\n",
    "        for j in range(1,4):\n",
    "            dataset[\"X_cov_{}{}\".format(i,j)] = 0\n",
    "            X_cov_list.append(\"X_cov_{}{}\".format(i,j))\n",
    "            for t in range(20):\n",
    "                dataset[\"X_cov_{}{}\".format(i,j)] += ((dataset[\"X_{}_roc_{}\".format(i,t)]\n",
    "                                                      - dataset[\"X_{}_roc20\".format(i)])\n",
    "                                                    * (dataset[\"X_{}_roc_{}\".format(j,t)] \n",
    "                                                    - dataset[\"X_{}_roc20\".format(j)]))\n",
    "    return X_cov_list\n",
    "list_indicators.append(X_cov)\n",
    "list_selection.append(\"X_roc_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "X_cov_list = X_cov(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Volatility of blended strategies\n",
    "# This function need \"I_cov\" appended to the dataset first\n",
    "def sigma(dataset):\n",
    "    dataset[\"sigma\"] = 0\n",
    "    for i in range(1,8):\n",
    "        for j in range(1,8):\n",
    "            dataset[\"sigma\"] += (252*dataset['weight_I_{}'.format(i)]*dataset['weight_I_{}'.format(j)]\n",
    "                                * dataset[\"I_cov_{}{}\".format(i,j)])\n",
    "    dataset[\"sigma\"] = np.sqrt(dataset[\"sigma\"])\n",
    "    dataset[\"sigma\"][dataset[\"sigma\"]<0.005] = 0.005\n",
    "    return [\"sigma\"]\n",
    "list_indicators.append(sigma)\n",
    "list_selection.append(\"sigma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "sigma_list = sigma(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Sharpe ratio of the portfolio (blended strategies)\n",
    "# This function need \"sigma\" and \"I_wr\" appended to the dataset first\n",
    "def SR(dataset):\n",
    "    dataset[\"SR\"] = dataset[\"I_wr\"]/dataset[\"sigma\"]\n",
    "    return [\"SR\"]\n",
    "list_indicators.append(SR)\n",
    "list_selection.append(\"SR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "SR_list = SR(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Shifted Sharpe ratios of the portfolio \n",
    "# This function need \"sigma\" and \"I_wr_shifted\" appended to the dataset first \n",
    "def SR_shifted(dataset):\n",
    "    SR_shifted_list = []\n",
    "    for s in range(1,4):\n",
    "        dataset[\"SR_S{}\".format(s)] = dataset[\"I_wr_S{}\".format(s)]/dataset[\"sigma\"]\n",
    "        SR_shifted_list.append(\"SR_S{}\".format(s))\n",
    "    return SR_shifted_list\n",
    "list_indicators.append(SR_shifted)\n",
    "list_selection.append(\"SR_shifted_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "SR_shifted_list = SR_shifted(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Sharpe raio of each strategy alone\n",
    "# This function need \"I_cov\" and \"I_roc5\" appended to the dataset first \n",
    "def SR_I(dataset):\n",
    "    SR_I_list = []\n",
    "    for i in range(1,8):\n",
    "        dataset[\"SR_I{}\".format(i)] = np.sqrt(dataset[\"I_cov_{}{}\".format(i,i)])\n",
    "        dataset[\"SR_I{}\".format(i)][dataset[\"SR_I{}\".format(i)]<0.005] = 0.005 \n",
    "        dataset[\"SR_I{}\".format(i)] = dataset[\"I_{}_roc5\".format(i)] / dataset[\"SR_I{}\".format(i)]\n",
    "        SR_I_list.append(\"SR_I{}\".format(i))\n",
    "    return SR_I_list\n",
    "list_indicators.append(SR_I)\n",
    "list_selection.append(\"SR_I_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "SR_I_list = SR_I(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# list of indicators operations to run over the dataset\n",
    "indicators = [i for i in list_indicators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def engineering(dataset, indicators):\n",
    "    ''' Apply feature engineering transormations to the dataset \n",
    "    '''\n",
    "    for indicator in indicators:\n",
    "        indicator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "engineering(X, indicators)\n",
    "engineering(test, indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I_roc_list',\n",
       " 'I_roc20_list',\n",
       " 'I_roc5_list',\n",
       " 'X_roc_list',\n",
       " 'X_roc20_list',\n",
       " 'X_roc5_list',\n",
       " 'I_roc5_shifted_list',\n",
       " 'I_wr',\n",
       " 'I_wr_shifted_list',\n",
       " 'I_cov_list',\n",
       " 'X_roc_list',\n",
       " 'sigma',\n",
       " 'SR',\n",
       " 'SR_shifted_list',\n",
       " 'SR_I_list']"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of all the functions created \n",
    "list_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = [\"weight_I_1\", \"weight_I_2\", \"weight_I_3\", \"weight_I_4\", \"weight_I_5\", \"weight_I_6\", \"weight_I_7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After different combination of features, I retain this one which give me the best score\n",
    "selection = weights_list+SR_I_list+I_roc20_list+X_roc20_list+SR_shifted_list+X_cov_list+SR_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline = Pipeline(\n",
    "#    [\n",
    "#        ('selector', SelectKBest(f_regression)),\n",
    "#        ('model', xgb)\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params_xgb = {'selector__k':[10,20,30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search = GridSearchCV(\n",
    "#    estimator = pipeline,\n",
    "#    param_grid = params_xgb,\n",
    "#    scoring = 'neg_mean_absolute_error',\n",
    "#    cv = 5\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object _BaseKFold.split at 0x1a3ae0d450>,\n",
       "             error_score=nan,\n",
       "             estimator=HistGradientBoostingRegressor(l2_regularization=0.15,\n",
       "                                                     learning_rate=0.1,\n",
       "                                                     loss='least_squares',\n",
       "                                                     max_bins=255, max_depth=2,\n",
       "                                                     max_iter=100,\n",
       "                                                     max_leaf_nodes=31,\n",
       "                                                     min_samples_leaf=20,\n",
       "                                                     n_iter_no_change=None,\n",
       "                                                     random_state=None,\n",
       "                                                     scoring=None, tol=1e-07,\n",
       "                                                     validation_fraction=0.1,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.2, 0.21],\n",
       "                         'max_iter': [50, 100, 150],\n",
       "                         'max_leaf_nodes': [15, 30, 50],\n",
       "                         'min_samples_leaf': [5, 8, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkf = GroupKFold(5)\n",
    "params = {\n",
    "    'learning_rate': [0.2, 0.21],\n",
    "    'min_samples_leaf': [5, 8, 10],\n",
    "    'max_iter': [50, 100, 150],\n",
    "    'max_leaf_nodes': [15, 30, 50]\n",
    "}\n",
    "\n",
    "search_gb = GridSearchCV(HistGradientBoostingRegressor(max_depth=2,\n",
    "                                                 l2_regularization=0.15),\n",
    "                      params,\n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      n_jobs= -1,\n",
    "                      cv=gkf.split(X.loc[:, :], y.smoothed, id_group))\n",
    "search_gb.fit(X[selection], y.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2,\n",
       " 'max_iter': 100,\n",
       " 'max_leaf_nodes': 15,\n",
       " 'min_samples_leaf': 10}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5693022395097986"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3689537876662926, 0.5782751818924042)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model = []\n",
    "train_res = []\n",
    "test_res = []\n",
    "\n",
    "gkf = GroupKFold(5)\n",
    "for i, (train_index, test_index) in enumerate(gkf.split(X.loc[:, :], y.smoothed, id_group)):\n",
    "        train_X_fold = X.iloc[train_index, :].loc[:, selection].values \n",
    "        train_Y_fold = y.smoothed.iloc[train_index].values\n",
    "        test_X_fold = X.iloc[test_index, :].loc[:, selection].values\n",
    "        test_Y_fold = y.smoothed.iloc[test_index].values\n",
    "               \n",
    "        gb = HistGradientBoostingRegressor(max_depth=2, l2_regularization=0.15,\n",
    "                                          min_samples_leaf=10, learning_rate=0.2,\n",
    "                                          max_leaf_nodes=15)\n",
    "        gb.fit(train_X_fold, train_Y_fold)\n",
    "        \n",
    "        train_preds = gb.predict(train_X_fold)\n",
    "        test_preds = gb.predict(test_X_fold)\n",
    "        mae_train = mean_absolute_error(train_Y_fold, train_preds)\n",
    "        mae_test = mean_absolute_error(test_Y_fold, test_preds)\n",
    "        train_res.append(mae_train)\n",
    "        test_res.append(mae_test)\n",
    "        \n",
    "        gb_model.append(gb)\n",
    "        \n",
    "np.mean(train_res), np.mean(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submission = gb.predict(test[selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"pred\"] = test_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"fpred\"] = np.sign(test['pred']) * np.exp(-1/abs(test['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.pred.to_csv(\"submissions/submission_gb.csv\")\n",
    "test = test.drop([\"pred\",\"fpred\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = []\n",
    "train_res = []\n",
    "test_res = []\n",
    "\n",
    "gkf = GroupKFold(5)\n",
    "for i, (train_index, test_index) in enumerate(gkf.split(X.loc[:, :], y.smoothed, id_group)):\n",
    "        train_X_fold = X.iloc[train_index, :].loc[:, selection].values\n",
    "        train_Y_fold = y.iloc[train_index].values\n",
    "        test_X_fold = X.iloc[test_index, :].loc[:, selection].values\n",
    "        test_Y_fold = y.iloc[test_index].values\n",
    "        \n",
    "        rf = RandomForestRegressor(criterion='mae', max_depth=2, n_jobs=-1)\n",
    "        rf.fit(train_X_fold, train_Y_fold)\n",
    "        \n",
    "        train_preds = rf.predict(train_X_fold)\n",
    "        test_preds = rf.predict(test_X_fold)\n",
    "        mae_train = mean_absolute_error(train_Y_fold, train_preds)\n",
    "        mae_test = mean_absolute_error(test_Y_fold, test_preds)\n",
    "        train_res.append(mae_train)\n",
    "        test_res.append(mae_test)\n",
    "        \n",
    "        rf_model.append(rf)\n",
    "        \n",
    "np.mean(train_res), np.mean(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of its time of training, the Random Forest model have been train in parallel on another computer\n",
    "# The model correspond to the one above\n",
    "# I upload the training here\n",
    "models = []\n",
    "for i in range(5):\n",
    "    models.append(load('./model_dir/' + 'model_' + str(i) + '.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code been used to save the model : \n",
    "# for i, model in enumerate(rf_model):\n",
    "#     dump(model, './model_dir/' + 'model_' + str(i) + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for model in models:\n",
    "    res = model.predict(X[selection])\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -0.536672\n",
       "1       0.404113\n",
       "2       0.425993\n",
       "3       0.348289\n",
       "4       0.597408\n",
       "          ...   \n",
       "9995    0.119318\n",
       "9996    0.287358\n",
       "9997    0.609562\n",
       "9998    0.569162\n",
       "9999   -0.045909\n",
       "Length: 10000, dtype: float64"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res_df = pd.DataFrame(results).T\n",
    "test_res_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47465698789036276"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y.smoothed, np.sign(test_res_df.mean(axis=1))*np.exp(-1/abs(np.sign(test_res_df.mean(axis=1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for model in models:\n",
    "    res = model.predict(test[selection])\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.550146\n",
       "1       0.152546\n",
       "2       0.312853\n",
       "3       0.283591\n",
       "4       0.064488\n",
       "          ...   \n",
       "4445    0.314462\n",
       "4446    0.180333\n",
       "4447    0.190934\n",
       "4448    0.128698\n",
       "4449    0.004498\n",
       "Length: 4450, dtype: float64"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res_df = pd.DataFrame(results).T\n",
    "test_res_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"pred\"] = test_res_df.mean(axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.pred.to_csv(\"submissions/submission_rf_mean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:46] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object _BaseKFold.split at 0x1a38315cd0>,\n",
       "             error_score=nan,\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=1, gamma=0,\n",
       "                                    importance_type='gain', learning_rate=0.1,\n",
       "                                    max_delta_step=0, max_depth=2,\n",
       "                                    min_child_weight=1, missing=None,\n",
       "                                    n_estimators=100, n_jobs=1, nthread=None,\n",
       "                                    objectiv..., random_state=0,\n",
       "                                    reg_alpha=0, reg_lambda=1,\n",
       "                                    scale_pos_weight=1, seed=None, silent=None,\n",
       "                                    subsample=1, verbosity=1),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.2, 0.21],\n",
       "                         'reg_alpha': [1, 95, 100, 150, 200],\n",
       "                         'reg_lambda': [1, 95, 100, 150, 200]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkf = GroupKFold(5)\n",
    "params = {\n",
    "    'learning_rate' : [0.01, 0.1, 0.2, 0.21],\n",
    "    #'max_depth' : [2,3],\n",
    "    'reg_alpha' : [1, 50, 95, 100, 150, 200],\n",
    "    'reg_lambda': [0.1, 1, 2],\n",
    "}\n",
    "\n",
    "search_xgb = GridSearchCV(XGBRegressor(max_depth=2,\n",
    "                                                 ),\n",
    "                      params,\n",
    "                      scoring='neg_mean_absolute_error', \n",
    "                      n_jobs= -1,\n",
    "                      cv=gkf.split(X.loc[:, :], y.smoothed, id_group))\n",
    "search_xgb.fit(X[selection], y.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'reg_alpha': 95, 'reg_lambda': 1}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.572466509438496"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_xgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:43:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:43:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:43:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:43:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:43:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.43935302144029703, 0.5569483189674088)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = []\n",
    "train_res = []\n",
    "test_res = []\n",
    "\n",
    "gkf = GroupKFold(5)\n",
    "for i, (train_index, test_index) in enumerate(gkf.split(X.loc[:, :], y.smoothed, id_group)):\n",
    "        train_X_fold = X.iloc[train_index, :].loc[:, selection].values\n",
    "        train_Y_fold = y.smoothed.iloc[train_index].values\n",
    "        test_X_fold = X.iloc[test_index, :].loc[:, selection].values\n",
    "        test_Y_fold = y.smoothed.iloc[test_index].values\n",
    "        \n",
    "        xgb = XGBRegressor(max_depth=2, n_jobs=-1, \n",
    "                           learning_rate = 0.2, \n",
    "                           reg_alpha = 80, \n",
    "                           reg_lambda = 1)\n",
    "        \n",
    "        xgb.fit(train_X_fold, train_Y_fold)\n",
    "        \n",
    "        train_preds = xgb.predict(train_X_fold)\n",
    "        test_preds = xgb.predict(test_X_fold)\n",
    "        mae_train = mean_absolute_error(train_Y_fold, train_preds)\n",
    "        mae_test = mean_absolute_error(test_Y_fold, test_preds)\n",
    "        train_res.append(mae_train)\n",
    "        test_res.append(mae_test)\n",
    "        \n",
    "        xgb_model.append(xgb)\n",
    "        \n",
    "np.mean(train_res), np.mean(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred_train = xgb.predict(X[selection].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"pred\"] = xgb_pred_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4753883545317744"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y.smoothed, X.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb.predict(test[selection].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"pred\"] = xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.pred.to_csv(\"submissions/submission_xgb_mean5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"fpred\"] = np.sign(test.pred)*np.exp(-1/abs(test.pred))\n",
    "test = test.drop([\"pred\",\"fpred\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Univariate model / adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_I1 = X_train.loc[:,\"I_1_lag_20\":\"I_1_lag_0\"]\n",
    "X_train_I2 = X_train.loc[:,\"I_2_lag_20\":\"I_2_lag_0\"]\n",
    "X_train_I3 = X_train.loc[:,\"I_3_lag_20\":\"I_3_lag_0\"]\n",
    "X_train_I4 = X_train.loc[:,\"I_4_lag_20\":\"I_4_lag_0\"]\n",
    "X_train_I5 = X_train.loc[:,\"I_5_lag_20\":\"I_5_lag_0\"]\n",
    "X_train_I6 = X_train.loc[:,\"I_6_lag_20\":\"I_6_lag_0\"]\n",
    "X_train_I7 = X_train.loc[:,\"I_7_lag_20\":\"I_7_lag_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_I1 = X_val.loc[:,\"I_1_lag_20\":\"I_1_lag_0\"]\n",
    "X_val_I2 = X_val.loc[:,\"I_2_lag_20\":\"I_2_lag_0\"]\n",
    "X_val_I3 = X_val.loc[:,\"I_3_lag_20\":\"I_3_lag_0\"]\n",
    "X_val_I4 = X_val.loc[:,\"I_4_lag_20\":\"I_4_lag_0\"]\n",
    "X_val_I5 = X_val.loc[:,\"I_5_lag_20\":\"I_5_lag_0\"]\n",
    "X_val_I6 = X_val.loc[:,\"I_6_lag_20\":\"I_6_lag_0\"]\n",
    "X_val_I7 = X_val.loc[:,\"I_7_lag_20\":\"I_7_lag_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_I1 = test.loc[:,\"I_1_lag_20\":\"I_1_lag_0\"]\n",
    "X_test_I2 = test.loc[:,\"I_2_lag_20\":\"I_2_lag_0\"]\n",
    "X_test_I3 = test.loc[:,\"I_3_lag_20\":\"I_3_lag_0\"]\n",
    "X_test_I4 = test.loc[:,\"I_4_lag_20\":\"I_4_lag_0\"]\n",
    "X_test_I5 = test.loc[:,\"I_5_lag_20\":\"I_5_lag_0\"]\n",
    "X_test_I6 = test.loc[:,\"I_6_lag_20\":\"I_6_lag_0\"]\n",
    "X_test_I7 = test.loc[:,\"I_7_lag_20\":\"I_7_lag_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_I1 = sc_x.fit_transform(X_train_I1)\n",
    "X_train_I2 = sc_x.fit_transform(X_train_I2)\n",
    "X_train_I3 = sc_x.fit_transform(X_train_I3)\n",
    "X_train_I4 = sc_x.fit_transform(X_train_I4)\n",
    "X_train_I5 = sc_x.fit_transform(X_train_I5)\n",
    "X_train_I6 = sc_x.fit_transform(X_train_I6)\n",
    "X_train_I7 = sc_x.fit_transform(X_train_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_I1 = sc_x.transform(X_val_I1)\n",
    "X_val_I2 = sc_x.transform(X_val_I2)\n",
    "X_val_I3 = sc_x.transform(X_val_I3)\n",
    "X_val_I4 = sc_x.transform(X_val_I4)\n",
    "X_val_I5 = sc_x.transform(X_val_I5)\n",
    "X_val_I6 = sc_x.transform(X_val_I6)\n",
    "X_val_I7 = sc_x.transform(X_val_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_I1 = sc_x.transform(X_test_I1)\n",
    "X_test_I2 = sc_x.transform(X_test_I2)\n",
    "X_test_I3 = sc_x.transform(X_test_I3)\n",
    "X_test_I4 = sc_x.transform(X_test_I4)\n",
    "X_test_I5 = sc_x.transform(X_test_I5)\n",
    "X_test_I6 = sc_x.transform(X_test_I6)\n",
    "X_test_I7 = sc_x.transform(X_test_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_I1 = np.expand_dims(X_train_I1, axis=-1)\n",
    "X_train_I2 = np.expand_dims(X_train_I2, axis=-1)\n",
    "X_train_I3 = np.expand_dims(X_train_I3, axis=-1)\n",
    "X_train_I4 = np.expand_dims(X_train_I4, axis=-1)\n",
    "X_train_I5 = np.expand_dims(X_train_I5, axis=-1)\n",
    "X_train_I6 = np.expand_dims(X_train_I6, axis=-1)\n",
    "X_train_I7 = np.expand_dims(X_train_I7, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_I1 = np.expand_dims(X_val_I1, axis=-1)\n",
    "X_val_I2 = np.expand_dims(X_val_I2, axis=-1)\n",
    "X_val_I3 = np.expand_dims(X_val_I3, axis=-1)\n",
    "X_val_I4 = np.expand_dims(X_val_I4, axis=-1)\n",
    "X_val_I5 = np.expand_dims(X_val_I5, axis=-1)\n",
    "X_val_I6 = np.expand_dims(X_val_I6, axis=-1)\n",
    "X_val_I7 = np.expand_dims(X_val_I7, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_I1 = np.expand_dims(X_test_I1, axis=-1)\n",
    "X_test_I2 = np.expand_dims(X_test_I2, axis=-1)\n",
    "X_test_I3 = np.expand_dims(X_test_I3, axis=-1)\n",
    "X_test_I4 = np.expand_dims(X_test_I4, axis=-1)\n",
    "X_test_I5 = np.expand_dims(X_test_I5, axis=-1)\n",
    "X_test_I6 = np.expand_dims(X_test_I6, axis=-1)\n",
    "X_test_I7 = np.expand_dims(X_test_I7, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 158\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_univariate_1 = tf.data.Dataset.from_tensor_slices((X_train_I1, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_2 = tf.data.Dataset.from_tensor_slices((X_train_I2, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_3 = tf.data.Dataset.from_tensor_slices((X_train_I3, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_4 = tf.data.Dataset.from_tensor_slices((X_train_I4, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_5 = tf.data.Dataset.from_tensor_slices((X_train_I5, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_6 = tf.data.Dataset.from_tensor_slices((X_train_I6, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_univariate_7 = tf.data.Dataset.from_tensor_slices((X_train_I7, y_train.smoothed)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_univariate_1 = tf.data.Dataset.from_tensor_slices((X_val_I1, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_2 = tf.data.Dataset.from_tensor_slices((X_val_I2, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_3 = tf.data.Dataset.from_tensor_slices((X_val_I3, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_4 = tf.data.Dataset.from_tensor_slices((X_val_I4, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_5 = tf.data.Dataset.from_tensor_slices((X_val_I5, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_6 = tf.data.Dataset.from_tensor_slices((X_val_I6, y_val.smoothed)).batch(BATCH_SIZE).repeat()\n",
    "val_univariate_7 = tf.data.Dataset.from_tensor_slices((X_val_I7, y_val.smoothed)).batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_univariate_1.take(1))\n",
    "y_train.smoothed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lstm_model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_5 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_6 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "simple_lstm_model_7 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=X_train_I1.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lstm_model_1.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_2.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_3.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_4.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_5.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_6.compile(optimizer='adam', loss='mae', metrics=[R2])\n",
    "simple_lstm_model_7.compile(optimizer='adam', loss='mae', metrics=[R2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, _ in val_univariate.take(1):\n",
    "    print(simple_lstm_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 10\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = simple_lstm_model_1.fit(train_univariate_1, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_1, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_2 = simple_lstm_model_2.fit(train_univariate_2, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_2, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_3 = simple_lstm_model_3.fit(train_univariate_3, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_3, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_4 = simple_lstm_model_4.fit(train_univariate_4, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_4, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_5 = simple_lstm_model_5.fit(train_univariate_5, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_5, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_6 = simple_lstm_model_6.fit(train_univariate_6, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_6, validation_steps=50)\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "history_7 = simple_lstm_model_7.fit(train_univariate_7, epochs=EPOCHS,\n",
    "                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                      validation_data=val_univariate_7, validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_2.history[\"loss\"], color=\"b\", label=\"Training Loss\")\n",
    "plt.plot(history_2.history[\"val_loss\"], color=\"r\", label=\"Validation Loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"R2\"], color=\"b\", label = \"Training R2\")\n",
    "plt.plot(history.history[\"val_R2\"], color=\"r\", label =\"Validation R2\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on train values\n",
    "lstm_pred_I1 = simple_lstm_model_1.predict(X_train_I1)\n",
    "lstm_pred_I2 = simple_lstm_model_2.predict(X_train_I2)\n",
    "lstm_pred_I3 = simple_lstm_model_3.predict(X_train_I3)\n",
    "lstm_pred_I4 = simple_lstm_model_4.predict(X_train_I4)\n",
    "lstm_pred_I5 = simple_lstm_model_5.predict(X_train_I5)\n",
    "lstm_pred_I6 = simple_lstm_model_6.predict(X_train_I6)\n",
    "lstm_pred_I7 = simple_lstm_model_7.predict(X_train_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on validation values\n",
    "lstm_val_pred_I1 = simple_lstm_model_1.predict(X_val_I1)\n",
    "lstm_val_pred_I2 = simple_lstm_model_2.predict(X_val_I2)\n",
    "lstm_val_pred_I3 = simple_lstm_model_3.predict(X_val_I3)\n",
    "lstm_val_pred_I4 = simple_lstm_model_4.predict(X_val_I4)\n",
    "lstm_val_pred_I5 = simple_lstm_model_5.predict(X_val_I5)\n",
    "lstm_val_pred_I6 = simple_lstm_model_6.predict(X_val_I6)\n",
    "lstm_val_pred_I7 = simple_lstm_model_7.predict(X_val_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on test values\n",
    "lstm_test_pred_I1 = simple_lstm_model_1.predict(X_test_I1)\n",
    "lstm_test_pred_I2 = simple_lstm_model_2.predict(X_test_I2)\n",
    "lstm_test_pred_I3 = simple_lstm_model_3.predict(X_test_I3)\n",
    "lstm_test_pred_I4 = simple_lstm_model_4.predict(X_test_I4)\n",
    "lstm_test_pred_I5 = simple_lstm_model_5.predict(X_test_I5)\n",
    "lstm_test_pred_I6 = simple_lstm_model_6.predict(X_test_I6)\n",
    "lstm_test_pred_I7 = simple_lstm_model_7.predict(X_test_I7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = {\"lstm_pred_I1\":lstm_pred_I1[:,0], \"lstm_pred_I2\":lstm_pred_I2[:,0], \"lstm_pred_I3\":lstm_pred_I3[:,0],\n",
    "    \"lstm_pred_I4\":lstm_pred_I4[:,0], \"lstm_pred_I5\":lstm_pred_I5[:,0], \"lstm_pred_I6\":lstm_pred_I6[:,0], \n",
    "     \"lstm_pred_I7\":lstm_pred_I7[:,0]}\n",
    "lstm_I_train = pd.DataFrame(data=d_train, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_wI_train = pd.concat([X_train.iloc[:,0:7], lstm_I_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_val = {\"lstm_pred_I1\":lstm_val_pred_I1[:,0], \"lstm_pred_I2\":lstm_val_pred_I2[:,0], \"lstm_pred_I3\":lstm_val_pred_I3[:,0],\n",
    "    \"lstm_pred_I4\":lstm_val_pred_I4[:,0], \"lstm_pred_I5\":lstm_val_pred_I5[:,0], \"lstm_pred_I6\":lstm_val_pred_I6[:,0], \n",
    "     \"lstm_pred_I7\":lstm_val_pred_I7[:,0]}\n",
    "lstm_I_val = pd.DataFrame(data=d_val, index=X_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_wI_val = pd.concat([X_val.iloc[:,0:7], lstm_I_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_wI_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = {\"lstm_pred_I1\":lstm_test_pred_I1[:,0], \"lstm_pred_I2\":lstm_test_pred_I2[:,0], \"lstm_pred_I3\":lstm_test_pred_I3[:,0],\n",
    "    \"lstm_pred_I4\":lstm_test_pred_I4[:,0], \"lstm_pred_I5\":lstm_test_pred_I5[:,0], \"lstm_pred_I6\":lstm_test_pred_I6[:,0], \n",
    "     \"lstm_pred_I7\":lstm_test_pred_I7[:,0]}\n",
    "lstm_I_test = pd.DataFrame(data=d_test, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_wI_test = pd.concat([test.iloc[:,0:7], lstm_I_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train an adaboost on the lstm predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = AdaBoostRegressor()\n",
    "adaboost.fit(lstm_wI_train, y_train.smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_train_pred = adaboost.predict(lstm_wI_train)\n",
    "ada_val_pred = adaboost.predict(lstm_wI_val)\n",
    "ada_test_pred = adaboost.predict(lstm_wI_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_wI_train[\"pred\"] = ada_train_pred\n",
    "lstm_wI_val[\"pred\"] = ada_val_pred\n",
    "lstm_wI_test[\"pred\"] = ada_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_wI_train[\"fpred\"] = np.sign(lstm_wI_train[\"pred\"])*np.exp(-1/abs(lstm_wI_train[\"pred\"]))\n",
    "lstm_wI_val[\"fpred\"] = np.sign(lstm_wI_val[\"pred\"])*np.exp(-1/abs(lstm_wI_val[\"pred\"]))\n",
    "lstm_wI_test[\"fpred\"] = np.sign(lstm_wI_test[\"pred\"])*np.exp(-1/abs(lstm_wI_test[\"pred\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(y_train.smoothed,lstm_wI_train[\"fpred\"]))\n",
    "print(mean_absolute_error(y_val.smoothed,lstm_wI_val[\"fpred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission of the lstm/adaboost model on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_wI_test.fpred.to_csv(\"submission_lstm_adaboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best score obtain on public leaderbord :\n",
    "Random Forest with score mae = 0.5388**\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
